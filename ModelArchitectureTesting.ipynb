{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57166f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n"
     ]
    }
   ],
   "source": [
    "#+.+\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "device=torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b341b",
   "metadata": {},
   "source": [
    "**INPUT SIZE AND RGB NORMALIZATION**\n",
    "\n",
    "RGB mean: 0.539095009313355, 0.5253846275760173, 0.47516918293895094 \n",
    "\n",
    "RGB std: 0.29325260016825044, 0.2845220684942233, 0.30522874839608877\n",
    "\n",
    "280x196 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc4d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define image transformations\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize((280,196)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5391,0.5254,0.4752],std=[0.2933,0.2845,0.3052])\n",
    "])\n",
    "\n",
    "dataset=ImageFolder(root='data/animals',transform=transform) #dataset[i][0]: image tensor (3,280,196), dataset[i][1]: image integer label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367a103",
   "metadata": {},
   "source": [
    "**TRAINING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fe3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for images,labels in train_loader: \n",
    "            images,labels=images.to(device),labels.to(device)\n",
    "            optimizer.zero_grad() #clear previous gradients\n",
    "            outputs=model(images) #forward pass\n",
    "            loss=criterion(outputs,labels) #compute loss\n",
    "            loss.backward() #backpropagate\n",
    "            optimizer.step() #update weights\n",
    "            train_loss+=loss.item()*images.size(0)\n",
    "        train_loss/=len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss=0.0\n",
    "        with torch.no_grad(): \n",
    "            for images,labels in val_loader:\n",
    "                images,labels=images.to(device),labels.to(device)\n",
    "                outputs=model(images)\n",
    "                loss=criterion(outputs,labels)\n",
    "                val_loss+=loss.item()*images.size(0)\n",
    "        val_loss/=len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:02d} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "        )\n",
    "    \n",
    "    model.eval()\n",
    "    correct,total=0,0\n",
    "    with torch.no_grad(): \n",
    "        for images,labels in test_loader: \n",
    "            images,labels=images.to(device),labels.to(device)\n",
    "            outputs=model(images)\n",
    "            predicted=outputs.argmax(dim=1)\n",
    "            total+=labels.size(0)\n",
    "            correct+=(predicted==labels).sum().item() \n",
    "    accuracy=correct/total\n",
    "    return f\"Test accuracy: {accuracy*100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf8867",
   "metadata": {},
   "source": [
    "**BASE MODEL**\n",
    "\n",
    "Input (3 x 280 x 196) \n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(3 → 32, kernel=3, padding=1) -> ReLU -> MaxPool2d(2x2): Output Block1(32 x 140 x 98)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(32 → 64, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block2(64 x 70 x 49)\n",
    "\n",
    "▼\n",
    "\n",
    "Flatten (64x70x49 = 219,520 features)\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 1: 219,520 → 256 -> ReLu\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 2: 256 → 90 logits\n",
    "\n",
    "▼\n",
    "\n",
    "Output (90 logits → one per animal class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd0c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module): \n",
    "    def __init__(self,num_classes=90): \n",
    "        super().__init__()\n",
    "        \n",
    "        #feature extraction\n",
    "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1) #1st convolution: 3 in channels (RGB), 32 out channels (\"pattern maps\"), kernel size of 3, padding of 1 [(3-1)//2]\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1) #2nd convolution: 32 in channels, 64 out channels, kernel size of 3\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2) #max pool: kernel size of 2 (every 2x2 region becomes 1 pixel, halving both width and height)\n",
    "        \n",
    "        #classifying\n",
    "        self.fc1=nn.Linear(64*70*49,256) #1st linear layer: input -> 64 channels * 70 width pixels * 49 height pixels, output -> 256 \"hidden features\" [fc: fully connected layer->every input node connected to every output node]\n",
    "        self.fc2=nn.Linear(256,num_classes) #2nd linear layer: input -> 256 \"hidden features\", output -> 90 desired animal classes\n",
    "        \n",
    "    def forward(self,x): #x->input batch\n",
    "        x=self.pool(F.relu(self.conv1(x))) #Block 1: Conv1->ReLu->Pool->Output (32,140,98)\n",
    "        x=self.pool(F.relu(self.conv2(x))) #Block 2: Conv2->ReLu->Pool->Output (64,70,49)\n",
    "        x=x.view(x.size(0),-1) #Flatten: (batch_size,64*70*49) -> ready for linear layers\n",
    "        x=F.relu(self.fc1(x)) #fc1 -> ReLu -> compressed to 256 hidden features\n",
    "        x=self.fc2(x) #fc2 -> output 90 logits (one per animal class)\n",
    "        \n",
    "        return x #return logits (class \"scores\"), logits fed into loss function in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07335868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzo/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.3977 | Val Loss: 4.1412 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 3.5412 | Val Loss: 3.5497 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 1.9284 | Val Loss: 3.3545 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 0.4809 | Val Loss: 3.9657 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 0.0839 | Val Loss: 4.3617 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 0.0383 | Val Loss: 4.4664 | LR: 1.50e-04\n",
      "Epoch 07 | Train Loss: 0.0122 | Val Loss: 4.5867 | LR: 1.50e-04\n",
      "Epoch 08 | Train Loss: 0.0070 | Val Loss: 4.6086 | LR: 1.50e-04\n",
      "Epoch 09 | Train Loss: 0.0033 | Val Loss: 4.7795 | LR: 7.50e-05\n",
      "Epoch 10 | Train Loss: 0.0052 | Val Loss: 4.7298 | LR: 7.50e-05\n",
      "Epoch 11 | Train Loss: 0.0036 | Val Loss: 4.7597 | LR: 7.50e-05\n",
      "Epoch 12 | Train Loss: 0.0027 | Val Loss: 4.8213 | LR: 3.75e-05\n",
      "Epoch 13 | Train Loss: 0.0028 | Val Loss: 4.8257 | LR: 3.75e-05\n",
      "Epoch 14 | Train Loss: 0.0019 | Val Loss: 4.8517 | LR: 3.75e-05\n",
      "Epoch 15 | Train Loss: 0.0019 | Val Loss: 4.8798 | LR: 1.87e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 33.29%'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=SimpleCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "train_size=int(0.7*len(dataset)) \n",
    "val_size=int(0.15*len(dataset))\n",
    "test_size=len(dataset)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(dataset,[train_size,val_size,test_size])\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=15 \n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a44888",
   "metadata": {},
   "source": [
    "Not the worst. Considering that random chance is ~1.1%, this still performs well for a base model. Despite loss being tiny, generalization is poor: likely overfitting. There are only 60 images per class which is very small for 90 classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc515d5",
   "metadata": {},
   "source": [
    "**RESNET18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29843db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzo/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/enzo/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 2.3593 | Val Loss: 1.5182 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 0.7423 | Val Loss: 1.0399 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 0.2258 | Val Loss: 0.8109 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 0.0866 | Val Loss: 0.6967 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 0.0478 | Val Loss: 0.6798 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 0.0229 | Val Loss: 0.6402 | LR: 3.00e-04\n",
      "Epoch 07 | Train Loss: 0.0142 | Val Loss: 0.6428 | LR: 3.00e-04\n",
      "Epoch 08 | Train Loss: 0.0801 | Val Loss: 1.8057 | LR: 3.00e-04\n",
      "Epoch 09 | Train Loss: 0.6577 | Val Loss: 1.3599 | LR: 1.50e-04\n",
      "Epoch 10 | Train Loss: 0.0829 | Val Loss: 0.7839 | LR: 1.50e-04\n",
      "Epoch 11 | Train Loss: 0.0161 | Val Loss: 0.7050 | LR: 1.50e-04\n",
      "Epoch 12 | Train Loss: 0.0121 | Val Loss: 0.7244 | LR: 7.50e-05\n",
      "Epoch 13 | Train Loss: 0.0069 | Val Loss: 0.7138 | LR: 7.50e-05\n",
      "Epoch 14 | Train Loss: 0.0062 | Val Loss: 0.7119 | LR: 7.50e-05\n",
      "Epoch 15 | Train Loss: 0.0046 | Val Loss: 0.6912 | LR: 3.75e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 82.12%'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models \n",
    "resnet18=models.resnet18(pretrained=True)\n",
    "resnet18.fc=nn.Linear(resnet18.fc.in_features,90)\n",
    "model=resnet18.to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "train_size=int(0.7*len(dataset)) \n",
    "val_size=int(0.15*len(dataset))\n",
    "test_size=len(dataset)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(dataset,[train_size,val_size,test_size])\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=15 \n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba7430",
   "metadata": {},
   "source": [
    "Wow, a much better performance from the pretrained Resnet18 model (it is actually very impressive to get almost 60% accuracy on a classification task involving 90 categories). The training loss drops steadily, although slower than the SimpleCNN it is more gradual which indicates stable learning. This likely means the model is learning meaningful features rather than just memorizing. \n",
    "\n",
    "Why it's so much better (maybe): \n",
    "\n",
    "- Residual connections use \n",
    "- Pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc790e",
   "metadata": {},
   "source": [
    "Let's try improving 'complexity' by making the CNN deeper with extra layers, thus increasing number of channels (more \"pattern maps\"). Our new \"deeper\" model will have more abstraction and pattern maps, while having fewer classifier parameters due to pooling in the extra layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed93fe",
   "metadata": {},
   "source": [
    "**DEEPER MODEL**\n",
    "\n",
    "Input (3 x 280 x 196) \n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(3 → 32, kernel=3, padding=1) -> ReLU -> MaxPool2d(2x2): Output Block1(32 x 140 x 98)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(32 → 64, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block2(64 x 70 x 49)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(64 → 128, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block3(128 x 35 x 24)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(128 → 256, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block3(256 x 17 x 12)\n",
    "\n",
    "▼\n",
    "\n",
    "Flatten (256x17x12 = 52,224 features)\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 1: 52,224 → 256 -> ReLu\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 2: 256 → 90 logits\n",
    "\n",
    "▼\n",
    "\n",
    "Output (90 logits → one per animal class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf6f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module): \n",
    "    def __init__(self,num_classes=90): \n",
    "        super().__init__()\n",
    "        \n",
    "        #feature extraction\n",
    "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1) #1st convolution: 3 in channels (RGB), 32 out channels (\"pattern maps\"), kernel size of 3, padding of 1 [(3-1)//2]\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1) #2nd convolution: 32 in channels, 64 out channels, kernel size of 3\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=3,padding=1) #3rd convolution: 64 in channels, 128 out channels, kernel size of 3\n",
    "        self.conv4=nn.Conv2d(128,256,kernel_size=3,padding=1) #4th convolution: 128 in channels, 256 out channels, kernel size of 3\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2) #max pool: kernel size of 2 (every 2x2 region becomes 1 pixel, halving both width and height)\n",
    "        \n",
    "        #classifying\n",
    "        self.fc1=nn.Linear(256*17*12,256) #1st linear layer: input -> 256 channels * 17 width pixels * 12 height pixels, output -> 256 \"hidden features\" [fc: fully connected layer->every input node connected to every output node]\n",
    "        self.fc2=nn.Linear(256,num_classes) #2nd linear layer: input -> 256 \"hidden features\", output -> 90 desired animal classes\n",
    "        \n",
    "    def forward(self,x): #x->input batch\n",
    "        x=self.pool(F.relu(self.conv1(x))) #Block 1: Conv1->ReLu->Pool->Output (32,140,98)\n",
    "        x=self.pool(F.relu(self.conv2(x))) #Block 2: Conv2->ReLu->Pool->Output (64,70,49)\n",
    "        x=self.pool(F.relu(self.conv3(x))) #Block 3: Conv3->ReLu->Pool->Output (128,35,24)\n",
    "        x=self.pool(F.relu(self.conv4(x))) #Block 4: Conv4->ReLu->Pool->Output (256,17,12)\n",
    "        x=x.view(x.size(0),-1) #Flatten: (batch_size,256*17*12) -> ready for linear layers\n",
    "        x=F.relu(self.fc1(x)) #fc1 -> ReLu -> compressed to 256 hidden features\n",
    "        x=self.fc2(x) #fc2 -> output 90 logits (one per animal class)\n",
    "        \n",
    "        return x #return logits (class \"scores\"), logits fed into loss function in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d60e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.3956 | Val Loss: 4.2000 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 3.9337 | Val Loss: 3.9235 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 3.3934 | Val Loss: 3.5780 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 2.6563 | Val Loss: 3.4527 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 1.6424 | Val Loss: 3.5866 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 0.6860 | Val Loss: 4.4548 | LR: 3.00e-04\n",
      "Epoch 07 | Train Loss: 0.2242 | Val Loss: 5.4324 | LR: 1.50e-04\n",
      "Epoch 08 | Train Loss: 0.0528 | Val Loss: 5.7543 | LR: 1.50e-04\n",
      "Epoch 09 | Train Loss: 0.0134 | Val Loss: 6.1493 | LR: 1.50e-04\n",
      "Epoch 10 | Train Loss: 0.0050 | Val Loss: 6.6876 | LR: 7.50e-05\n",
      "Epoch 11 | Train Loss: 0.0031 | Val Loss: 6.7426 | LR: 7.50e-05\n",
      "Epoch 12 | Train Loss: 0.0013 | Val Loss: 6.9017 | LR: 7.50e-05\n",
      "Epoch 13 | Train Loss: 0.0011 | Val Loss: 7.0458 | LR: 3.75e-05\n",
      "Epoch 14 | Train Loss: 0.0009 | Val Loss: 7.1213 | LR: 3.75e-05\n",
      "Epoch 15 | Train Loss: 0.0008 | Val Loss: 7.2008 | LR: 3.75e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 38.47%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=DeeperCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "train_size=int(0.7*len(dataset)) \n",
    "val_size=int(0.15*len(dataset))\n",
    "test_size=len(dataset)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(dataset,[train_size,val_size,test_size])\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=15 \n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f888a",
   "metadata": {},
   "source": [
    "Solid improvement! Looks like adding those two extra conv layers helped improve our model. But we are ambitious, so we want to try and match ResNet18. Let's make the network even deeper. We will also add a dropout to prevent overfitting during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acbe2f",
   "metadata": {},
   "source": [
    "**SUPER DEEP MODEL**\n",
    "\n",
    "Input (3 x 280 x 196) \n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(3 → 32, kernel=3, padding=1) -> ReLU -> MaxPool2d(2x2): Output Block1(32 x 140 x 98)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(32 → 64, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block2(64 x 70 x 49)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(64 → 128, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block3(128 x 35 x 24)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(128 → 256, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block3(256 x 17 x 12)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(256 → 380, kernel=3, padding=1) -> ReLu -> MaxPool2d(2x2): Output Block3(380 x 8 x 6)\n",
    "\n",
    "▼\n",
    "\n",
    "Flatten (380x8x6 = 18,240 features)\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 1: 18,240 → 256 -> ReLu -> Dropout\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer 2: 256 → 90 logits\n",
    "\n",
    "▼\n",
    "\n",
    "Output (90 logits → one per animal class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc66683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperDeepCNN(nn.Module): \n",
    "    def __init__(self,num_classes=90): \n",
    "        super().__init__()\n",
    "        \n",
    "        #feature extraction\n",
    "        self.conv1=nn.Conv2d(3,32,kernel_size=3,padding=1) #1st convolution: 3 in channels (RGB), 32 out channels (\"pattern maps\"), kernel size of 3, padding of 1 [(3-1)//2]\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1) #2nd convolution: 32 in channels, 64 out channels, kernel size of 3\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=3,padding=1) #3rd convolution: 64 in channels, 128 out channels, kernel size of 3\n",
    "        self.conv4=nn.Conv2d(128,256,kernel_size=3,padding=1) #4th convolution: 128 in channels, 256 out channels, kernel size of 3\n",
    "        self.conv5=nn.Conv2d(256,380,kernel_size=3,padding=1) #5th convolution: 256 in channels, 380 out channels, kernel size of 3 (don't want to huge of a jump or too many features)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2) #max pool: kernel size of 2 (every 2x2 region becomes 1 pixel, halving both width and height)\n",
    "        self.dropout=nn.Dropout(0.05) #dropout to reduce overfitting (10% chance to dropout, since small dataset)\n",
    "        \n",
    "        #classifying\n",
    "        self.fc1=nn.Linear(380*8*6,256) #1st linear layer: input -> 256 channels (after adaptive avg pool), output -> 256 \"hidden features\" [fc: fully connected layer->every input node connected to every output node]\n",
    "        self.fc2=nn.Linear(256,num_classes) #2nd linear layer: input -> 256 \"hidden features\", output -> 90 desired animal classes\n",
    "        \n",
    "    def forward(self,x): #x->input batch\n",
    "        x=self.pool(F.relu(self.conv1(x))) #Block 1: Conv1->ReLu->Pool->Output (32,140,98)\n",
    "        x=self.pool(F.relu(self.conv2(x))) #Block 2: Conv2->ReLu->Pool->Output (64,70,49)\n",
    "        x=self.pool(F.relu(self.conv3(x))) #Block 3: Conv3->ReLu->Pool->Output (128,35,24)\n",
    "        x=self.pool(F.relu(self.conv4(x))) #Block 4: Conv4->ReLu->Pool->Output (256,17,12)\n",
    "        x=self.pool(F.relu(self.conv5(x))) #Block 5: Conv5->ReLu->Pool->Output (380,8,6)\n",
    "        x=x.view(x.size(0),-1) #Flatten: (batch_size,380*8*6) -> ready for linear layers\n",
    "        x=self.dropout(F.relu(self.fc1(x))) #fc1 -> ReLu -> Dropout -> compressed to 256 hidden features\n",
    "        x=self.fc2(x) #fc2 -> output 90 logits (one per animal class)\n",
    "        \n",
    "        return x #return logits (class \"scores\"), logits fed into loss function in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbdf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.4958 | Val Loss: 4.4652 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 4.3076 | Val Loss: 4.1547 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 3.9464 | Val Loss: 3.9209 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 3.5479 | Val Loss: 3.8328 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 3.0380 | Val Loss: 3.6415 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 2.4450 | Val Loss: 3.6924 | LR: 3.00e-04\n",
      "Epoch 07 | Train Loss: 1.7482 | Val Loss: 3.8234 | LR: 3.00e-04\n",
      "Epoch 08 | Train Loss: 1.1708 | Val Loss: 4.4452 | LR: 1.50e-04\n",
      "Epoch 09 | Train Loss: 0.5387 | Val Loss: 5.3855 | LR: 1.50e-04\n",
      "Epoch 10 | Train Loss: 0.3060 | Val Loss: 5.9011 | LR: 1.50e-04\n",
      "Epoch 11 | Train Loss: 0.1815 | Val Loss: 6.8939 | LR: 7.50e-05\n",
      "Epoch 12 | Train Loss: 0.1105 | Val Loss: 7.3357 | LR: 7.50e-05\n",
      "Epoch 13 | Train Loss: 0.0882 | Val Loss: 7.8356 | LR: 7.50e-05\n",
      "Epoch 14 | Train Loss: 0.0691 | Val Loss: 8.1619 | LR: 3.75e-05\n",
      "Epoch 15 | Train Loss: 0.0611 | Val Loss: 7.9583 | LR: 3.75e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 33.05%'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=SuperDeepCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "train_size=int(0.7*len(dataset)) \n",
    "val_size=int(0.15*len(dataset))\n",
    "test_size=len(dataset)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(dataset,[train_size,val_size,test_size])\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=15 \n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d0793a",
   "metadata": {},
   "source": [
    "Performance didn't improve, so this pretty much confirms that the core issue is not depth but generalization. Also looks like dropout actually hurt our model's performance, probably due to the dataset being really small. Of course it is difficult to match ResNet18 because we have a tiny dataset for training CNNs from scratch, but we can try to implement some features that will improve generalization: \n",
    "\n",
    "- Global average pooling (forces generalization by removing spatial dependence-averaging each feature map spatially, and avoid massive flattening and fully-connected layers)\n",
    "- Batch normalization (stabilizing feature scales)\n",
    "- Edited the channel growth pattern (to somewhat match  ResNet)\n",
    "- Removing dropout (hurt performance previously)\n",
    "\n",
    "We'll call it \"Modern\" CNN because it implements some modern techniques rather than just the simple CNN we started off with. Final caveat: GAP seems to learn slow, we need way more epochs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cac372",
   "metadata": {},
   "source": [
    "**SUPER DEEP MODEL**\n",
    "\n",
    "Input (3 x 280 x 196) \n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(3 → 64, kernel=3, padding=1) -> BatchNorm -> ReLU -> MaxPool2d(2x2): Output Block1(64 x 140 x 98)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(64 → 128, kernel=3, padding=1) -> BatchNorm -> ReLu -> MaxPool2d(2x2): Output Block2(128 x 70 x 49)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(128 → 256, kernel=3, padding=1) -> BatchNorm -> ReLu -> MaxPool2d(2x2): Output Block3(256 x 35 x 24)\n",
    "\n",
    "▼\n",
    "\n",
    "Conv2d(256 → 512, kernel=3, padding=1) -> BatchNorm -> ReLu -> MaxPool2d(2x2): Output Block3(512 x 17 x 12)\n",
    "\n",
    "▼\n",
    "\n",
    "Global Average Pooling -> Output (512x1x1)\n",
    "\n",
    "▼\n",
    "\n",
    "Flatten (512 features)\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer: 512 → 90 logits\n",
    "\n",
    "▼\n",
    "\n",
    "Output (90 logits → one per animal class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19aa35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernCNN(nn.Module):\n",
    "    def __init__(self,num_classes=90):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_ch,out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch,out_ch,3,padding=1,bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "            \n",
    "        self.features=nn.Sequential(\n",
    "            block(3,64),\n",
    "            block(64,128),\n",
    "            block(128,256),\n",
    "            block(256,512)\n",
    "        )\n",
    "\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "794050d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.1837 | Val Loss: 3.9282 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 3.8571 | Val Loss: 3.7651 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 3.6904 | Val Loss: 3.7018 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 3.5422 | Val Loss: 3.6323 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 3.4368 | Val Loss: 3.5228 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 3.3396 | Val Loss: 3.5625 | LR: 3.00e-04\n",
      "Epoch 07 | Train Loss: 3.2361 | Val Loss: 3.4189 | LR: 3.00e-04\n",
      "Epoch 08 | Train Loss: 3.1725 | Val Loss: 3.4354 | LR: 3.00e-04\n",
      "Epoch 09 | Train Loss: 3.1026 | Val Loss: 3.2923 | LR: 3.00e-04\n",
      "Epoch 10 | Train Loss: 3.0074 | Val Loss: 3.3124 | LR: 3.00e-04\n",
      "Epoch 11 | Train Loss: 2.9826 | Val Loss: 3.2374 | LR: 3.00e-04\n",
      "Epoch 12 | Train Loss: 2.9025 | Val Loss: 3.2409 | LR: 3.00e-04\n",
      "Epoch 13 | Train Loss: 2.8304 | Val Loss: 3.1916 | LR: 3.00e-04\n",
      "Epoch 14 | Train Loss: 2.7811 | Val Loss: 3.1398 | LR: 3.00e-04\n",
      "Epoch 15 | Train Loss: 2.7344 | Val Loss: 3.1976 | LR: 3.00e-04\n",
      "Epoch 16 | Train Loss: 2.6854 | Val Loss: 3.0312 | LR: 3.00e-04\n",
      "Epoch 17 | Train Loss: 2.6172 | Val Loss: 3.0494 | LR: 3.00e-04\n",
      "Epoch 18 | Train Loss: 2.5812 | Val Loss: 3.0826 | LR: 3.00e-04\n",
      "Epoch 19 | Train Loss: 2.5146 | Val Loss: 2.9421 | LR: 3.00e-04\n",
      "Epoch 20 | Train Loss: 2.4660 | Val Loss: 3.0285 | LR: 3.00e-04\n",
      "Epoch 21 | Train Loss: 2.4457 | Val Loss: 2.9850 | LR: 3.00e-04\n",
      "Epoch 22 | Train Loss: 2.3861 | Val Loss: 2.9626 | LR: 1.50e-04\n",
      "Epoch 23 | Train Loss: 2.2205 | Val Loss: 2.7723 | LR: 1.50e-04\n",
      "Epoch 24 | Train Loss: 2.1663 | Val Loss: 2.7936 | LR: 1.50e-04\n",
      "Epoch 25 | Train Loss: 2.1400 | Val Loss: 2.7327 | LR: 1.50e-04\n",
      "Epoch 26 | Train Loss: 2.1026 | Val Loss: 2.7777 | LR: 1.50e-04\n",
      "Epoch 27 | Train Loss: 2.0687 | Val Loss: 2.7309 | LR: 1.50e-04\n",
      "Epoch 28 | Train Loss: 2.0380 | Val Loss: 2.8356 | LR: 1.50e-04\n",
      "Epoch 29 | Train Loss: 2.0087 | Val Loss: 2.6861 | LR: 1.50e-04\n",
      "Epoch 30 | Train Loss: 1.9814 | Val Loss: 2.7152 | LR: 1.50e-04\n",
      "Epoch 31 | Train Loss: 1.9497 | Val Loss: 2.7158 | LR: 1.50e-04\n",
      "Epoch 32 | Train Loss: 1.9333 | Val Loss: 2.7839 | LR: 7.50e-05\n",
      "Epoch 33 | Train Loss: 1.8419 | Val Loss: 2.6135 | LR: 7.50e-05\n",
      "Epoch 34 | Train Loss: 1.7953 | Val Loss: 2.5783 | LR: 7.50e-05\n",
      "Epoch 35 | Train Loss: 1.7877 | Val Loss: 2.5609 | LR: 7.50e-05\n",
      "Epoch 36 | Train Loss: 1.7615 | Val Loss: 2.5771 | LR: 7.50e-05\n",
      "Epoch 37 | Train Loss: 1.7371 | Val Loss: 2.5437 | LR: 7.50e-05\n",
      "Epoch 38 | Train Loss: 1.7247 | Val Loss: 2.5618 | LR: 7.50e-05\n",
      "Epoch 39 | Train Loss: 1.6993 | Val Loss: 2.5397 | LR: 7.50e-05\n",
      "Epoch 40 | Train Loss: 1.6833 | Val Loss: 2.5320 | LR: 7.50e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 44.51%'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=ModernCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "train_size=int(0.7*len(dataset)) \n",
    "val_size=int(0.15*len(dataset))\n",
    "test_size=len(dataset)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(dataset,[train_size,val_size,test_size])\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=40 #more epochs\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813d4cd",
   "metadata": {},
   "source": [
    "Clearly a huge improvement, couple final touches: \n",
    "- add additional random changes to training input images, known as data augmentation \n",
    "- push to 70 epochs, appears train & val loss are still steadily going down i.e the model is still learning at 40 epochs (a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4235822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform=transforms.Compose([ #augmentation\n",
    "    transforms.RandomResizedCrop( #random crops of part of images\n",
    "        size=(280,196),\n",
    "        scale=(0.8,1.0),\n",
    "        ratio=(0.9,1.1)\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.4), #random horizontal flip\n",
    "    transforms.ColorJitter( #\n",
    "        brightness=0.25,\n",
    "        contrast=0.25,\n",
    "        saturation=0.25,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.RandomRotation(degrees=10), #random rotations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5391,0.5254,0.4752],\n",
    "        std=[0.2933,0.2845,0.3052]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "val_transform=transforms.Compose([ #no augmentation\n",
    "    transforms.Resize((280,196)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5391,0.5254,0.4752],std=[0.2933,0.2845,0.3052])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ace4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzo/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.1970 | Val Loss: 3.9432 | LR: 3.00e-04\n",
      "Epoch 02 | Train Loss: 3.8873 | Val Loss: 3.7987 | LR: 3.00e-04\n",
      "Epoch 03 | Train Loss: 3.7190 | Val Loss: 3.7511 | LR: 3.00e-04\n",
      "Epoch 04 | Train Loss: 3.5743 | Val Loss: 3.6096 | LR: 3.00e-04\n",
      "Epoch 05 | Train Loss: 3.4452 | Val Loss: 3.5503 | LR: 3.00e-04\n",
      "Epoch 06 | Train Loss: 3.3538 | Val Loss: 3.4450 | LR: 3.00e-04\n",
      "Epoch 07 | Train Loss: 3.2656 | Val Loss: 3.3704 | LR: 3.00e-04\n",
      "Epoch 08 | Train Loss: 3.2156 | Val Loss: 3.3501 | LR: 3.00e-04\n",
      "Epoch 09 | Train Loss: 3.1299 | Val Loss: 3.2460 | LR: 3.00e-04\n",
      "Epoch 10 | Train Loss: 3.0317 | Val Loss: 3.3797 | LR: 3.00e-04\n",
      "Epoch 11 | Train Loss: 2.9818 | Val Loss: 3.3305 | LR: 3.00e-04\n",
      "Epoch 12 | Train Loss: 2.9306 | Val Loss: 3.1517 | LR: 3.00e-04\n",
      "Epoch 13 | Train Loss: 2.8827 | Val Loss: 3.1437 | LR: 3.00e-04\n",
      "Epoch 14 | Train Loss: 2.8216 | Val Loss: 3.1311 | LR: 3.00e-04\n",
      "Epoch 15 | Train Loss: 2.7695 | Val Loss: 3.1209 | LR: 3.00e-04\n",
      "Epoch 16 | Train Loss: 2.6871 | Val Loss: 3.1014 | LR: 3.00e-04\n",
      "Epoch 17 | Train Loss: 2.6523 | Val Loss: 3.3725 | LR: 3.00e-04\n",
      "Epoch 18 | Train Loss: 2.6208 | Val Loss: 2.9584 | LR: 3.00e-04\n",
      "Epoch 19 | Train Loss: 2.5741 | Val Loss: 2.9398 | LR: 3.00e-04\n",
      "Epoch 20 | Train Loss: 2.5238 | Val Loss: 2.8971 | LR: 3.00e-04\n",
      "Epoch 21 | Train Loss: 2.4781 | Val Loss: 3.1139 | LR: 3.00e-04\n",
      "Epoch 22 | Train Loss: 2.4543 | Val Loss: 2.8739 | LR: 3.00e-04\n",
      "Epoch 23 | Train Loss: 2.3804 | Val Loss: 3.0105 | LR: 3.00e-04\n",
      "Epoch 24 | Train Loss: 2.3464 | Val Loss: 2.8725 | LR: 3.00e-04\n",
      "Epoch 25 | Train Loss: 2.2974 | Val Loss: 2.9150 | LR: 3.00e-04\n",
      "Epoch 26 | Train Loss: 2.2599 | Val Loss: 2.8948 | LR: 3.00e-04\n",
      "Epoch 27 | Train Loss: 2.2108 | Val Loss: 2.8384 | LR: 3.00e-04\n",
      "Epoch 28 | Train Loss: 2.1672 | Val Loss: 2.7758 | LR: 3.00e-04\n",
      "Epoch 29 | Train Loss: 2.1447 | Val Loss: 2.9075 | LR: 3.00e-04\n",
      "Epoch 30 | Train Loss: 2.1093 | Val Loss: 2.8257 | LR: 3.00e-04\n",
      "Epoch 31 | Train Loss: 2.0787 | Val Loss: 2.7171 | LR: 3.00e-04\n",
      "Epoch 32 | Train Loss: 2.0133 | Val Loss: 2.7868 | LR: 3.00e-04\n",
      "Epoch 33 | Train Loss: 2.0173 | Val Loss: 2.8088 | LR: 3.00e-04\n",
      "Epoch 34 | Train Loss: 1.9087 | Val Loss: 2.6882 | LR: 3.00e-04\n",
      "Epoch 35 | Train Loss: 1.9429 | Val Loss: 2.7483 | LR: 3.00e-04\n",
      "Epoch 36 | Train Loss: 1.9093 | Val Loss: 2.7783 | LR: 3.00e-04\n",
      "Epoch 37 | Train Loss: 1.8421 | Val Loss: 2.6338 | LR: 3.00e-04\n",
      "Epoch 38 | Train Loss: 1.8021 | Val Loss: 2.7680 | LR: 3.00e-04\n",
      "Epoch 39 | Train Loss: 1.7633 | Val Loss: 2.7993 | LR: 3.00e-04\n",
      "Epoch 40 | Train Loss: 1.7370 | Val Loss: 2.7030 | LR: 1.50e-04\n",
      "Epoch 41 | Train Loss: 1.5733 | Val Loss: 2.4875 | LR: 1.50e-04\n",
      "Epoch 42 | Train Loss: 1.5044 | Val Loss: 2.5322 | LR: 1.50e-04\n",
      "Epoch 43 | Train Loss: 1.4771 | Val Loss: 2.4689 | LR: 1.50e-04\n",
      "Epoch 44 | Train Loss: 1.4594 | Val Loss: 2.4631 | LR: 1.50e-04\n",
      "Epoch 45 | Train Loss: 1.4522 | Val Loss: 2.4891 | LR: 1.50e-04\n",
      "Epoch 46 | Train Loss: 1.4181 | Val Loss: 2.4173 | LR: 1.50e-04\n",
      "Epoch 47 | Train Loss: 1.3916 | Val Loss: 2.4611 | LR: 1.50e-04\n",
      "Epoch 48 | Train Loss: 1.3586 | Val Loss: 2.5052 | LR: 1.50e-04\n",
      "Epoch 49 | Train Loss: 1.3501 | Val Loss: 2.4636 | LR: 7.50e-05\n",
      "Epoch 50 | Train Loss: 1.2507 | Val Loss: 2.3453 | LR: 7.50e-05\n",
      "Epoch 51 | Train Loss: 1.2187 | Val Loss: 2.3560 | LR: 7.50e-05\n",
      "Epoch 52 | Train Loss: 1.1977 | Val Loss: 2.3753 | LR: 7.50e-05\n",
      "Epoch 53 | Train Loss: 1.1778 | Val Loss: 2.3509 | LR: 3.75e-05\n",
      "Epoch 54 | Train Loss: 1.1358 | Val Loss: 2.3161 | LR: 3.75e-05\n",
      "Epoch 55 | Train Loss: 1.1263 | Val Loss: 2.3347 | LR: 3.75e-05\n",
      "Epoch 56 | Train Loss: 1.1064 | Val Loss: 2.3496 | LR: 3.75e-05\n",
      "Epoch 57 | Train Loss: 1.1148 | Val Loss: 2.2904 | LR: 3.75e-05\n",
      "Epoch 58 | Train Loss: 1.0983 | Val Loss: 2.3182 | LR: 3.75e-05\n",
      "Epoch 59 | Train Loss: 1.0874 | Val Loss: 2.2814 | LR: 3.75e-05\n",
      "Epoch 60 | Train Loss: 1.0813 | Val Loss: 2.3002 | LR: 3.75e-05\n",
      "Epoch 61 | Train Loss: 1.0829 | Val Loss: 2.3405 | LR: 3.75e-05\n",
      "Epoch 62 | Train Loss: 1.0541 | Val Loss: 2.3081 | LR: 1.87e-05\n",
      "Epoch 63 | Train Loss: 1.0414 | Val Loss: 2.2770 | LR: 1.87e-05\n",
      "Epoch 64 | Train Loss: 1.0307 | Val Loss: 2.2753 | LR: 1.87e-05\n",
      "Epoch 65 | Train Loss: 1.0109 | Val Loss: 2.2777 | LR: 1.87e-05\n",
      "Epoch 66 | Train Loss: 1.0389 | Val Loss: 2.2814 | LR: 1.87e-05\n",
      "Epoch 67 | Train Loss: 1.0170 | Val Loss: 2.2798 | LR: 9.37e-06\n",
      "Epoch 68 | Train Loss: 1.0043 | Val Loss: 2.2734 | LR: 9.37e-06\n",
      "Epoch 69 | Train Loss: 1.0038 | Val Loss: 2.2735 | LR: 9.37e-06\n",
      "Epoch 70 | Train Loss: 0.9984 | Val Loss: 2.2546 | LR: 9.37e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 47.10%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=ModernCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n",
    "imgs=ImageFolder(root='data/animals')\n",
    "train_size=int(0.7*len(imgs))\n",
    "val_size=int(0.15*len(imgs))\n",
    "test_size=len(imgs)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(imgs,[train_size,val_size,test_size])\n",
    "train_dataset.dataset.transform=train_transform #augmentation\n",
    "val_dataset.dataset.transform=val_transform #no augmentation\n",
    "test_dataset.dataset.transform=val_transform #no augmentation\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=70 #more more epochs\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5c99d",
   "metadata": {},
   "source": [
    "Hmm, looks like learning is still very healthy but we are plateauing in test accuracy. Seems we are in the diminishing returns territory: yes more epochs leads to slightly better accuracy, but the tradeoff is probably not worth the computational cost and time. Last thing we'll add to try and replicate ResNet18: residual connections. These are meant to make optimization easier by allowing the model to learn small improvements or do nothing (if can't improve, just pass through unchanged). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af6818",
   "metadata": {},
   "source": [
    "**RESIDUAL CNN MODEL**\n",
    "\n",
    "Input (3 x 280 x 196) \n",
    "\n",
    "▼\n",
    "\n",
    "Residual Block 1: \n",
    "Conv2d(3 → 64, kernel=3, padding=1) -> BatchNorm -> ReLU -> Conv2d(64->64, kernel=3, padding=1) -> BatchNorm\n",
    "- skip connection from input -> ReLU -> MaxPool2d(2x2) -> OutputBlock1: (64x140x98)\n",
    "\n",
    "▼\n",
    "\n",
    "Residual Block 2: \n",
    "Conv2d(64 → 128, kernel=3, padding=1) -> BatchNorm -> ReLU -> Conv2d(128->128, kernel=3, padding=1) -> BatchNorm\n",
    "- skip connection from input -> ReLU -> MaxPool2d(2x2) -> OutputBlock1: (128x70x49)\n",
    "\n",
    "▼\n",
    "\n",
    "Residual Block 3: \n",
    "Conv2d(128 → 256, kernel=3, padding=1) -> BatchNorm -> ReLU -> Conv2d(256->256, kernel=3, padding=1) -> BatchNorm\n",
    "- skip connection from input -> ReLU -> MaxPool2d(2x2) -> OutputBlock1: (256x35x24)\n",
    "▼\n",
    "\n",
    "Residual Block 4: \n",
    "Conv2d(256 → 512, kernel=3, padding=1) -> BatchNorm -> ReLU -> Conv2d(512->512, kernel=3, padding=1) -> BatchNorm\n",
    "- skip connection from input -> ReLU -> MaxPool2d(2x2) -> OutputBlock1: (512x17x12)\n",
    "▼\n",
    "\n",
    "Global Average Pooling -> Output (512x1x1)\n",
    "\n",
    "▼\n",
    "\n",
    "Flatten (512 features)\n",
    "\n",
    "▼\n",
    "\n",
    "Linear Layer: 512 → 90 logits\n",
    "\n",
    "▼\n",
    "\n",
    "Output (90 logits → one per animal class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd2a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module): \n",
    "    def __init__(self,in_ch,out_ch): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(in_ch,out_ch,3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch,out_ch,3,padding=1,bias=False), #second convolution now in each block to refine features before adding skip connection\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "        \n",
    "        self.skip=nn.Identity() #skip path returns input unchanged\n",
    "        if in_ch!=out_ch: #if number of channels change\n",
    "            self.skip=nn.Conv2d(in_ch,out_ch,1,bias=False) #use 1x1 convolution to change channel count to keep spatial size the same\n",
    "        self.relu=nn.ReLU(inplace=True) #activation applied after combining learned features with input\n",
    "        self.pool=nn.MaxPool2d(2) #downsample spatial size by 2 after residual addition\n",
    "        \n",
    "    def forward(self,x): \n",
    "        out=self.conv(x) #run input through main convolutional path\n",
    "        skip=self.skip(x) #run input through skip connection \n",
    "        out=out+skip #residual additionL out=out+skip\n",
    "        out=self.relu(out) #apply non-linearity after combining\n",
    "        out=self.pool(out) #downsample\n",
    "        return out\n",
    "\n",
    "class ResCNN(nn.Module):\n",
    "    def __init__(self,num_classes=90):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.features=nn.Sequential(\n",
    "            ResBlock(3,64),\n",
    "            ResBlock(64,128),\n",
    "            ResBlock(128,256),\n",
    "            ResBlock(256,512)\n",
    "        )\n",
    "\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee14b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzo/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 4.2715 | Val Loss: 4.1918 | LR: 5.00e-04\n",
      "Epoch 02 | Train Loss: 3.9739 | Val Loss: 3.9381 | LR: 5.00e-04\n",
      "Epoch 03 | Train Loss: 3.8254 | Val Loss: 3.8254 | LR: 5.00e-04\n",
      "Epoch 04 | Train Loss: 3.6688 | Val Loss: 3.7668 | LR: 5.00e-04\n",
      "Epoch 05 | Train Loss: 3.5388 | Val Loss: 3.6260 | LR: 5.00e-04\n",
      "Epoch 06 | Train Loss: 3.4335 | Val Loss: 3.5542 | LR: 5.00e-04\n",
      "Epoch 07 | Train Loss: 3.3093 | Val Loss: 3.5257 | LR: 5.00e-04\n",
      "Epoch 08 | Train Loss: 3.1909 | Val Loss: 3.3854 | LR: 5.00e-04\n",
      "Epoch 09 | Train Loss: 3.0546 | Val Loss: 3.3859 | LR: 5.00e-04\n",
      "Epoch 10 | Train Loss: 2.9657 | Val Loss: 3.2277 | LR: 5.00e-04\n",
      "Epoch 11 | Train Loss: 2.8593 | Val Loss: 3.3798 | LR: 5.00e-04\n",
      "Epoch 12 | Train Loss: 2.7415 | Val Loss: 3.1699 | LR: 5.00e-04\n",
      "Epoch 13 | Train Loss: 2.6596 | Val Loss: 3.1426 | LR: 5.00e-04\n",
      "Epoch 14 | Train Loss: 2.5601 | Val Loss: 3.1145 | LR: 5.00e-04\n",
      "Epoch 15 | Train Loss: 2.4611 | Val Loss: 3.0294 | LR: 5.00e-04\n",
      "Epoch 16 | Train Loss: 2.3907 | Val Loss: 3.1369 | LR: 5.00e-04\n",
      "Epoch 17 | Train Loss: 2.2874 | Val Loss: 3.1056 | LR: 5.00e-04\n",
      "Epoch 18 | Train Loss: 2.2102 | Val Loss: 2.9700 | LR: 5.00e-04\n",
      "Epoch 19 | Train Loss: 2.1106 | Val Loss: 2.9674 | LR: 5.00e-04\n",
      "Epoch 20 | Train Loss: 2.0298 | Val Loss: 2.9042 | LR: 5.00e-04\n",
      "Epoch 21 | Train Loss: 1.9428 | Val Loss: 2.8720 | LR: 5.00e-04\n",
      "Epoch 22 | Train Loss: 1.8585 | Val Loss: 2.8030 | LR: 5.00e-04\n",
      "Epoch 23 | Train Loss: 1.7789 | Val Loss: 2.8192 | LR: 5.00e-04\n",
      "Epoch 24 | Train Loss: 1.7307 | Val Loss: 2.7365 | LR: 5.00e-04\n",
      "Epoch 25 | Train Loss: 1.6351 | Val Loss: 2.7111 | LR: 5.00e-04\n",
      "Epoch 26 | Train Loss: 1.5528 | Val Loss: 2.9459 | LR: 5.00e-04\n",
      "Epoch 27 | Train Loss: 1.4999 | Val Loss: 2.6880 | LR: 5.00e-04\n",
      "Epoch 28 | Train Loss: 1.3793 | Val Loss: 2.7953 | LR: 5.00e-04\n",
      "Epoch 29 | Train Loss: 1.3402 | Val Loss: 2.5889 | LR: 5.00e-04\n",
      "Epoch 30 | Train Loss: 1.2742 | Val Loss: 2.9324 | LR: 5.00e-04\n",
      "Epoch 31 | Train Loss: 1.1871 | Val Loss: 2.6016 | LR: 5.00e-04\n",
      "Epoch 32 | Train Loss: 1.1365 | Val Loss: 2.6489 | LR: 2.50e-04\n",
      "Epoch 33 | Train Loss: 0.8724 | Val Loss: 2.4424 | LR: 2.50e-04\n",
      "Epoch 34 | Train Loss: 0.7918 | Val Loss: 2.4478 | LR: 2.50e-04\n",
      "Epoch 35 | Train Loss: 0.7468 | Val Loss: 2.4863 | LR: 2.50e-04\n",
      "Epoch 36 | Train Loss: 0.7080 | Val Loss: 2.4720 | LR: 1.25e-04\n",
      "Epoch 37 | Train Loss: 0.5762 | Val Loss: 2.4510 | LR: 1.25e-04\n",
      "Epoch 38 | Train Loss: 0.5395 | Val Loss: 2.4543 | LR: 1.25e-04\n",
      "Epoch 39 | Train Loss: 0.5159 | Val Loss: 2.4435 | LR: 6.25e-05\n",
      "Epoch 40 | Train Loss: 0.4466 | Val Loss: 2.4175 | LR: 6.25e-05\n",
      "Epoch 41 | Train Loss: 0.4389 | Val Loss: 2.3833 | LR: 6.25e-05\n",
      "Epoch 42 | Train Loss: 0.4248 | Val Loss: 2.4537 | LR: 6.25e-05\n",
      "Epoch 43 | Train Loss: 0.4123 | Val Loss: 2.4507 | LR: 6.25e-05\n",
      "Epoch 44 | Train Loss: 0.3944 | Val Loss: 2.4000 | LR: 3.13e-05\n",
      "Epoch 45 | Train Loss: 0.3685 | Val Loss: 2.4648 | LR: 3.13e-05\n",
      "Epoch 46 | Train Loss: 0.3592 | Val Loss: 2.4683 | LR: 3.13e-05\n",
      "Epoch 47 | Train Loss: 0.3586 | Val Loss: 2.4188 | LR: 1.56e-05\n",
      "Epoch 48 | Train Loss: 0.3303 | Val Loss: 2.3688 | LR: 1.56e-05\n",
      "Epoch 49 | Train Loss: 0.3323 | Val Loss: 2.4951 | LR: 1.56e-05\n",
      "Epoch 50 | Train Loss: 0.3284 | Val Loss: 2.4169 | LR: 1.56e-05\n",
      "Epoch 51 | Train Loss: 0.3312 | Val Loss: 2.4318 | LR: 7.81e-06\n",
      "Epoch 52 | Train Loss: 0.3214 | Val Loss: 2.4365 | LR: 7.81e-06\n",
      "Epoch 53 | Train Loss: 0.3117 | Val Loss: 2.4328 | LR: 7.81e-06\n",
      "Epoch 54 | Train Loss: 0.3145 | Val Loss: 2.4739 | LR: 3.91e-06\n",
      "Epoch 55 | Train Loss: 0.3091 | Val Loss: 2.4443 | LR: 3.91e-06\n",
      "Epoch 56 | Train Loss: 0.3080 | Val Loss: 2.4499 | LR: 3.91e-06\n",
      "Epoch 57 | Train Loss: 0.3138 | Val Loss: 2.4461 | LR: 1.95e-06\n",
      "Epoch 58 | Train Loss: 0.3079 | Val Loss: 2.4613 | LR: 1.95e-06\n",
      "Epoch 59 | Train Loss: 0.3035 | Val Loss: 2.4115 | LR: 1.95e-06\n",
      "Epoch 60 | Train Loss: 0.3046 | Val Loss: 2.4373 | LR: 9.77e-07\n",
      "Epoch 61 | Train Loss: 0.3036 | Val Loss: 2.4354 | LR: 9.77e-07\n",
      "Epoch 62 | Train Loss: 0.3064 | Val Loss: 2.4296 | LR: 9.77e-07\n",
      "Epoch 63 | Train Loss: 0.2965 | Val Loss: 2.4805 | LR: 4.88e-07\n",
      "Epoch 64 | Train Loss: 0.3035 | Val Loss: 2.4330 | LR: 4.88e-07\n",
      "Epoch 65 | Train Loss: 0.2924 | Val Loss: 2.4273 | LR: 4.88e-07\n",
      "Epoch 66 | Train Loss: 0.3043 | Val Loss: 2.4776 | LR: 2.44e-07\n",
      "Epoch 67 | Train Loss: 0.3010 | Val Loss: 2.4322 | LR: 2.44e-07\n",
      "Epoch 68 | Train Loss: 0.3033 | Val Loss: 2.4265 | LR: 2.44e-07\n",
      "Epoch 69 | Train Loss: 0.3066 | Val Loss: 2.4163 | LR: 1.22e-07\n",
      "Epoch 70 | Train Loss: 0.3040 | Val Loss: 2.4281 | LR: 1.22e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 56.23%'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train inputs\n",
    "model=ResCNN(num_classes=90).to(device)\n",
    "criterion=nn.CrossEntropyLoss() #loss function for multi-class classification\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=5e-4) #slightly increase LR to start\n",
    "imgs=ImageFolder(root='data/animals')\n",
    "train_size=int(0.7*len(imgs))\n",
    "val_size=int(0.15*len(imgs))\n",
    "test_size=len(imgs)-train_size-val_size\n",
    "train_dataset,val_dataset,test_dataset=random_split(imgs,[train_size,val_size,test_size])\n",
    "train_dataset.dataset.transform=train_transform #augmentation\n",
    "val_dataset.dataset.transform=val_transform #no augmentation\n",
    "test_dataset.dataset.transform=val_transform #no augmentation\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "epochs=70 #more more epochs\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau( #'scheduler'\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "train_model(model,train_loader,val_loader,test_loader,optimizer,criterion,device,epochs,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd0206",
   "metadata": {},
   "source": [
    "Considering our severe limitations here: \n",
    "\n",
    "- Small dataset\n",
    "- Building model from scratch \n",
    "\n",
    "This is pretty good! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fca4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"ResCNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db189b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
